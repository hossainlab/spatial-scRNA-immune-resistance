{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalization, HVG Selection, and Dimensionality Reduction\n",
    "\n",
    "## Overview\n",
    "This notebook performs normalization, highly variable gene (HVG) selection, and dimensionality reduction on QC-filtered scRNA-seq data.\n",
    "\n",
    "### Objectives\n",
    "1. Normalize counts (library size normalization + log transformation)\n",
    "2. Identify highly variable genes\n",
    "3. Scale data for PCA\n",
    "4. Perform PCA and UMAP\n",
    "5. Initial clustering for QC assessment\n",
    "\n",
    "### Key Concepts\n",
    "- **Normalization**: Correct for differences in sequencing depth between cells\n",
    "- **HVG selection**: Focus on genes with high biological variability\n",
    "- **PCA**: Reduce dimensionality while preserving variance\n",
    "- **UMAP**: Non-linear embedding for visualization\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scanpy as sc\n",
    "import anndata as ad\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import yaml\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Scanpy settings\n",
    "sc.settings.verbosity = 3\n",
    "sc.settings.set_figure_params(dpi=100, facecolor='white')\n",
    "\n",
    "# Project paths\n",
    "PROJECT_ROOT = Path(\"../..\").resolve()\n",
    "DATA_PROCESSED = PROJECT_ROOT / 'data' / 'processed' / 'scrna'\n",
    "FIGURES = PROJECT_ROOT / 'results' / 'figures'\n",
    "CONFIG_PATH = PROJECT_ROOT / 'config' / 'analysis_params.yaml'\n",
    "\n",
    "# Load configuration\n",
    "with open(CONFIG_PATH, 'r') as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "# Set random seed\n",
    "SEED = config['random_seed']\n",
    "np.random.seed(SEED)\n",
    "\n",
    "print(f\"Random seed: {SEED}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display relevant parameters\n",
    "print(\"Normalization parameters:\")\n",
    "print(f\"  Target sum: {config['normalization']['target_sum']}\")\n",
    "print(f\"  Log transform: {config['normalization']['log_transform']}\")\n",
    "\n",
    "print(\"\\nFeature selection parameters:\")\n",
    "print(f\"  N top genes: {config['feature_selection']['n_top_genes']}\")\n",
    "print(f\"  Flavor: {config['feature_selection']['flavor']}\")\n",
    "\n",
    "print(\"\\nDimensionality reduction parameters:\")\n",
    "print(f\"  N PCs: {config['dim_reduction']['n_pcs']}\")\n",
    "print(f\"  N neighbors: {config['dim_reduction']['n_neighbors']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Filtered Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load QC-filtered data\n",
    "geo_id = \"GSE115978\"  # Modify for each dataset\n",
    "input_path = DATA_PROCESSED / f'{geo_id}_filtered.h5ad'\n",
    "\n",
    "# Check if file exists\n",
    "if input_path.exists():\n",
    "    adata = sc.read_h5ad(input_path)\n",
    "    print(f\"Loaded: {input_path}\")\n",
    "    print(f\"Cells: {adata.n_obs}\")\n",
    "    print(f\"Genes: {adata.n_vars}\")\n",
    "else:\n",
    "    print(f\"File not found: {input_path}\")\n",
    "    print(\"Please run 02a_qc_filtering.ipynb first\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Normalization\n",
    "\n",
    "We use library size normalization followed by log transformation:\n",
    "1. Normalize each cell to have the same total counts (target_sum)\n",
    "2. Log transform: log(1 + x)\n",
    "\n",
    "This is the standard scanpy workflow and is appropriate for most analyses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store raw counts\n",
    "adata.layers['counts'] = adata.X.copy()\n",
    "\n",
    "# Normalize to target sum\n",
    "sc.pp.normalize_total(\n",
    "    adata,\n",
    "    target_sum=config['normalization']['target_sum']\n",
    ")\n",
    "\n",
    "# Log transform\n",
    "if config['normalization']['log_transform']:\n",
    "    sc.pp.log1p(adata)\n",
    "\n",
    "# Store normalized counts\n",
    "adata.layers['normalized'] = adata.X.copy()\n",
    "\n",
    "print(\"Normalization complete\")\n",
    "print(f\"Stored layers: {list(adata.layers.keys())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Highly Variable Gene Selection\n",
    "\n",
    "Identify genes with high cell-to-cell variability, which are likely to be biologically informative.\n",
    "\n",
    "### Methods\n",
    "- **seurat_v3**: Models mean-variance relationship and selects genes with high residual variance\n",
    "- **cell_ranger**: Uses dispersion-based selection\n",
    "\n",
    "For integration across batches, consider using `batch_key` parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify highly variable genes\n",
    "sc.pp.highly_variable_genes(\n",
    "    adata,\n",
    "    n_top_genes=config['feature_selection']['n_top_genes'],\n",
    "    flavor=config['feature_selection']['flavor'],\n",
    "    # batch_key=config['feature_selection']['batch_key'],  # Uncomment for batch-aware HVG\n",
    ")\n",
    "\n",
    "n_hvg = adata.var['highly_variable'].sum()\n",
    "print(f\"\\nSelected {n_hvg} highly variable genes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize HVG selection\n",
    "sc.pl.highly_variable_genes(adata, save=f'_{geo_id}_hvg.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show top HVGs\n",
    "hvg_df = adata.var[adata.var['highly_variable']].sort_values('dispersions_norm', ascending=False)\n",
    "print(\"\\nTop 20 highly variable genes:\")\n",
    "print(hvg_df.head(20)[['means', 'dispersions', 'dispersions_norm']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Scaling\n",
    "\n",
    "Scale each gene to unit variance. This is required for PCA.\n",
    "\n",
    "**Note**: Scaling is only applied to HVGs to reduce memory usage. The full matrix remains unscaled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regress out unwanted sources of variation (optional)\n",
    "# Uncomment if needed:\n",
    "# sc.pp.regress_out(adata, ['total_counts', 'pct_counts_mt'])\n",
    "\n",
    "# Scale data\n",
    "sc.pp.scale(adata, max_value=10)\n",
    "\n",
    "print(\"Scaling complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Principal Component Analysis (PCA)\n",
    "\n",
    "Reduce dimensionality while preserving variance. Use only HVGs for PCA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run PCA\n",
    "sc.tl.pca(\n",
    "    adata,\n",
    "    n_comps=config['dim_reduction']['n_pcs'],\n",
    "    use_highly_variable=True,\n",
    "    svd_solver='arpack'\n",
    ")\n",
    "\n",
    "print(f\"PCA complete: {adata.obsm['X_pca'].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize variance explained\n",
    "sc.pl.pca_variance_ratio(\n",
    "    adata,\n",
    "    n_pcs=50,\n",
    "    log=True,\n",
    "    save=f'_{geo_id}_pca_variance.png'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine number of PCs to use\n",
    "# Look for elbow in variance explained\n",
    "cumvar = np.cumsum(adata.uns['pca']['variance_ratio'])\n",
    "\n",
    "# Find PC that captures 90% variance\n",
    "n_pcs_90 = np.argmax(cumvar >= 0.9) + 1\n",
    "print(f\"\\nPCs needed for 90% variance: {n_pcs_90}\")\n",
    "\n",
    "# We'll use the configured number of PCs\n",
    "n_pcs = config['dim_reduction']['n_pcs']\n",
    "print(f\"Using {n_pcs} PCs for downstream analysis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize PCA\n",
    "sc.pl.pca(\n",
    "    adata,\n",
    "    color=['total_counts', 'n_genes_by_counts', 'pct_counts_mt'],\n",
    "    ncols=3,\n",
    "    save=f'_{geo_id}_pca_qc.png'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Neighborhood Graph and UMAP\n",
    "\n",
    "Build a neighborhood graph in PCA space and compute UMAP embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute neighborhood graph\n",
    "sc.pp.neighbors(\n",
    "    adata,\n",
    "    n_neighbors=config['dim_reduction']['n_neighbors'],\n",
    "    n_pcs=n_pcs,\n",
    "    random_state=SEED\n",
    ")\n",
    "\n",
    "print(\"Neighborhood graph computed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute UMAP\n",
    "sc.tl.umap(adata, random_state=SEED)\n",
    "\n",
    "print(\"UMAP computed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize UMAP colored by QC metrics\n",
    "sc.pl.umap(\n",
    "    adata,\n",
    "    color=['total_counts', 'n_genes_by_counts', 'pct_counts_mt', 'pct_counts_ribo'],\n",
    "    ncols=2,\n",
    "    save=f'_{geo_id}_umap_qc.png'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Initial Clustering\n",
    "\n",
    "Perform initial clustering to assess data quality and identify major cell populations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Leiden clustering at multiple resolutions\n",
    "resolutions = config['clustering']['resolution']\n",
    "\n",
    "for res in resolutions:\n",
    "    key = f'leiden_{res}'\n",
    "    sc.tl.leiden(adata, resolution=res, key_added=key, random_state=SEED)\n",
    "    n_clusters = adata.obs[key].nunique()\n",
    "    print(f\"Resolution {res}: {n_clusters} clusters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize clustering results\n",
    "default_res = config['clustering']['default_resolution']\n",
    "default_key = f'leiden_{default_res}'\n",
    "\n",
    "sc.pl.umap(\n",
    "    adata,\n",
    "    color=[default_key],\n",
    "    legend_loc='on data',\n",
    "    title=f'Leiden clustering (res={default_res})',\n",
    "    save=f'_{geo_id}_umap_clusters.png'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare multiple resolutions\n",
    "sc.pl.umap(\n",
    "    adata,\n",
    "    color=[f'leiden_{r}' for r in [0.3, 0.5, 1.0]],\n",
    "    ncols=3,\n",
    "    save=f'_{geo_id}_umap_resolutions.png'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Marker Gene Visualization\n",
    "\n",
    "Check expression of known marker genes to validate cell populations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define marker genes from config\n",
    "markers = config['annotation']['markers']\n",
    "\n",
    "# Flatten marker dictionary\n",
    "all_markers = []\n",
    "for cell_type, genes in markers.items():\n",
    "    all_markers.extend(genes)\n",
    "\n",
    "# Filter to genes present in data\n",
    "present_markers = [g for g in all_markers if g in adata.var_names]\n",
    "missing_markers = [g for g in all_markers if g not in adata.var_names]\n",
    "\n",
    "print(f\"Markers present: {len(present_markers)}/{len(all_markers)}\")\n",
    "if missing_markers:\n",
    "    print(f\"Missing: {missing_markers[:10]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize key markers\n",
    "key_markers = ['CD3D', 'CD8A', 'CD4', 'FOXP3', 'CD14', 'CD68', 'MS4A1', 'EPCAM', 'PECAM1']\n",
    "key_markers = [m for m in key_markers if m in adata.var_names]\n",
    "\n",
    "if key_markers:\n",
    "    sc.pl.umap(\n",
    "        adata,\n",
    "        color=key_markers,\n",
    "        ncols=3,\n",
    "        save=f'_{geo_id}_umap_markers.png'\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Save Processed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save processed data with all embeddings\n",
    "output_path = DATA_PROCESSED / f'{geo_id}_processed.h5ad'\n",
    "adata.write(output_path)\n",
    "\n",
    "print(f\"Saved processed data to: {output_path}\")\n",
    "print(f\"\\nData contains:\")\n",
    "print(f\"  Cells: {adata.n_obs}\")\n",
    "print(f\"  Genes: {adata.n_vars}\")\n",
    "print(f\"  HVGs: {adata.var['highly_variable'].sum()}\")\n",
    "print(f\"  Layers: {list(adata.layers.keys())}\")\n",
    "print(f\"  Embeddings: {list(adata.obsm.keys())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Summary\n",
    "\n",
    "### Processing Steps Completed\n",
    "1. Library size normalization (target_sum=10,000)\n",
    "2. Log transformation\n",
    "3. HVG selection (n=3,000, seurat_v3 method)\n",
    "4. Scaling\n",
    "5. PCA (50 components)\n",
    "6. Neighborhood graph (15 neighbors)\n",
    "7. UMAP embedding\n",
    "8. Leiden clustering (multiple resolutions)\n",
    "\n",
    "### Data Structure\n",
    "- `adata.X`: Normalized, log-transformed, scaled expression\n",
    "- `adata.layers['counts']`: Raw counts\n",
    "- `adata.layers['normalized']`: Normalized log counts (before scaling)\n",
    "- `adata.obsm['X_pca']`: PCA embedding\n",
    "- `adata.obsm['X_umap']`: UMAP embedding\n",
    "\n",
    "### Next Steps\n",
    "1. Run doublet detection in `02c_doublet_detection.ipynb`\n",
    "2. Proceed to integration in `03_integration/`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
